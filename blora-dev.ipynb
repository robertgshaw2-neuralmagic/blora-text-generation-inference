{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Running Example 0 // 1\n",
    "You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
    "### INPUT:\n",
    "```json\n",
    "{\"instructions\": \"Explain what an alpaca is\"}\n",
    "```\n",
    "### OUTPUT:\n",
    "```json\n",
    "{\"response\": \"An alpaca is a South American camelid species related to llamas. They are usually found in the Andes Mountains of Peru, Bolivia, Ecuador, and Chile. Alpacas are known for their soft fleece, which is used for making clothing, blankets, and other textiles. They are also used for meat and dairy products.\"}\n",
    "```\n",
    "### OUTPUT:\n",
    "```json\n",
    "{\"inst\n",
    "### INPUT:\n",
    "```json\n",
    "{\"instructions\": \"Explain what an alpaca is\"}\n",
    "```\n",
    "### OUTPUT:\n",
    "```json\n",
    "{\"response\": \"An alpaca is a South American camelid species related to llamas. They are usually found in the Andes mountain range of Peru, Bolivia, Ecuador, and Chile. Alpacas are known for their soft fleece, which is used to make clothing, blankets, and other textiles. They are also used for their meat and as a source of wool.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_MEMORY_FRACTION\"] = \"0.5\"\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'server-dev'\n",
      "/usr/src/server-dev\n",
      "# Compile protos\n",
      "pip install grpcio-tools==1.51.1 mypy-protobuf==3.4.0 'types-protobuf>=3.20.4' --no-cache-dir\n",
      "Requirement already satisfied: grpcio-tools==1.51.1 in /opt/conda/lib/python3.9/site-packages (1.51.1)\n",
      "Requirement already satisfied: mypy-protobuf==3.4.0 in /opt/conda/lib/python3.9/site-packages (3.4.0)\n",
      "Requirement already satisfied: types-protobuf>=3.20.4 in /opt/conda/lib/python3.9/site-packages (4.23.0.2)\n",
      "Requirement already satisfied: grpcio>=1.51.1 in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (1.56.0)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (4.23.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (68.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mmkdir text_generation_server/pb || true\n",
      "mkdir: cannot create directory ‘text_generation_server/pb’: File exists\n",
      "python -m grpc_tools.protoc -I../proto --python_out=text_generation_server/pb \\\n",
      "\t--grpc_python_out=text_generation_server/pb --mypy_out=text_generation_server/pb ../proto/generate.proto\n",
      "/opt/conda/lib/python3.9/site-packages/grpc_tools/protoc.py:21: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "Writing mypy to generate_pb2.pyi\n",
      "find text_generation_server/pb/ -type f -name \"*.py\" -print0 -exec sed -i -e 's/^\\(import.*pb2\\)/from . \\1/g' {} \\;\n",
      "text_generation_server/pb/generate_pb2_grpc.py\u0000text_generation_server/pb/__init__.py\u0000text_generation_server/pb/generate_pb2.py\u0000touch text_generation_server/pb/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%cd server-dev\n",
    "!make gen-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q_proj', 'k_proj', 'v_proj', 'o_proj']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from text_generation_server.models.flash_llama import FlashLlama\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = FlashLlama(model_id=model_id, dtype=torch.float16, quantize=\"bitsandbytes-nf4\")\n",
    "\n",
    "lora_id = \"nealchandra/llama-2-7b-hf-lora-alpaca-json\"\n",
    "peft_config = LoraConfig.from_pretrained(lora_id)\n",
    "print(peft_config.target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 16])\n",
      "torch.Size([16, 4096])\n",
      "torch.Size([4096, 16])\n",
      "torch.Size([16, 4096])\n",
      "torch.Size([16, 4096])\n",
      "{'q_proj': 0, 'k_proj': 4096, 'v_proj': 8192}\n",
      "{'q_proj': 4096, 'k_proj': 8192, 'v_proj': 12288}\n"
     ]
    }
   ],
   "source": [
    "from text_generation_server.models.blora_flash_llama import BLoraFlashLlama\n",
    "\n",
    "blora_llama = BLoraFlashLlama(model, {lora_id: LoraConfig.from_pretrained(lora_id)})\n",
    "\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.o_proj.linear.lora_A[\"o_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.o_proj.linear.lora_B[\"o_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.lora_A[\"q_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.lora_B[\"q_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.lora_B[\"q_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.start_out_indexes)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.end_out_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "from text_generation_server.models.flash_causal_lm import FlashCausalLMBatch\n",
    "from text_generation_server.pb import generate_pb2\n",
    "\n",
    "max_input_length = 128\n",
    "max_batch_size = 2\n",
    "max_prefill_tokens = max_input_length * max_batch_size - 32\n",
    "\n",
    "warmup_requests = []\n",
    "n_tokens = 0\n",
    "while n_tokens < max_prefill_tokens:\n",
    "    warmup_requests.append(\n",
    "        generate_pb2.Request(\n",
    "            id=0,\n",
    "            inputs=\"_text\" * max_input_length,\n",
    "            truncate=min(max_input_length, max_prefill_tokens - n_tokens),\n",
    "            parameters=generate_pb2.NextTokenChooserParameters(\n",
    "                do_sample=False\n",
    "            ),\n",
    "            stopping_parameters=generate_pb2.StoppingCriteriaParameters(\n",
    "                max_new_tokens=2\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    n_tokens += max_input_length\n",
    "\n",
    "warmup_batch = generate_pb2.Batch(id=0, requests=warmup_requests, size=len(warmup_requests))\n",
    "\n",
    "fclm_warmup_batch = FlashCausalLMBatch.from_pb(\n",
    "    pb=warmup_batch,\n",
    "    tokenizer=model.tokenizer,\n",
    "    dtype=model.dtype,\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "blora_llama.set_batch_ids(lora_ids=[\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"]*max_batch_size, cu_seqlen_prefill=fclm_warmup_batch.cu_seqlen_prefill)\n",
    "max_supported_total_tokens = blora_llama.model.warmup(batch=fclm_warmup_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### INPUT:\n",
      "```json\n",
      "{\"instructions\": \"Explain what an alpaca is\"}\n",
      "```\n",
      "### OUTPUT:\n",
      "```json\n",
      "{\"response\": \"An alpaca is a South American camelid species related to llamas. They are usually found in the Andes mountain range of Peru, Bolivia, Ecuador, and Chile. Alpacas are known for their soft fleece, which is used to make clothing, blankets, and other textiles. They are also used for their meat and as a source of wool.\"}\n",
      "```\n",
      "### INPUT:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = generate_pb2.NextTokenChooserParameters(\n",
    "    watermark=False,\n",
    "    temperature=1.0,\n",
    "    repetition_penalty=1.0,\n",
    "    top_k=0,\n",
    "    top_p=1.0,\n",
    "    typical_p=1.0,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "stopping_parameters = generate_pb2.StoppingCriteriaParameters(\n",
    "    max_new_tokens=100,\n",
    "    ignore_eos_token=True\n",
    ")\n",
    "\n",
    "input_lst = [\n",
    "    '### INPUT:\\n```json\\n{\"instructions\": \"Explain what an alpaca is\"}\\n```\\n### OUTPUT:\\n',\n",
    "    # '### INPUT:\\n```json\\n{\"instructions\": \"Describe what deep learning is\"}\\n```\\n### OUTPUT:\\n'\n",
    "]\n",
    "\n",
    "requests = [\n",
    "    generate_pb2.Request(\n",
    "        id=idx,\n",
    "        inputs=inputs,\n",
    "        truncate=max_input_length,\n",
    "        parameters=parameters,    \n",
    "        stopping_parameters=stopping_parameters\n",
    "    )\n",
    "    for idx, inputs in enumerate(input_lst)\n",
    "]\n",
    "\n",
    "fclm_batch = FlashCausalLMBatch.from_pb(\n",
    "    pb=generate_pb2.Batch(id=0, requests=requests),\n",
    "    tokenizer=model.tokenizer,\n",
    "    dtype=model.dtype,\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "texts = {\n",
    "    idx: request.inputs\n",
    "    for idx, request in enumerate(fclm_batch.requests)\n",
    "}\n",
    "\n",
    "blora_llama.set_batch_ids(lora_ids=[\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"], cu_seqlen_prefill=fclm_batch.cu_seqlen_prefill)\n",
    "\n",
    "for _ in range(100):\n",
    "    generations, fclm_batch = blora_llama.model.generate_token(fclm_batch)\n",
    "    for idx, gen in enumerate(generations):\n",
    "        texts[idx] += gen.token_text\n",
    "\n",
    "print(texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"An alpaca is a South American camelid species related to llamas. They are usually found in the Andes Mountains of Peru, Bolivia, Ecuador, and Chile. Alpacas are known for their soft fleece, which is used for making clothing, blankets, and other textiles. They are also used for meat and dairy products.\"\n",
    "\"An alpaca is a South American camelid species related to llamas. They are usually found in the Andes Mountains of Peru, Bolivia, Ecuador, and Chile. Alpacas are known for their soft fleece, which is used for making clothing, blankets, and other textiles. They are also used for meat and dairy products.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a0eeb585604a8887ab439642234b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig \n",
    "\n",
    "token = os.environ[\"HUGGING_FACE_HUB_TOKEN\"]\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "lora_id = \"nealchandra/llama-2-7b-hf-lora-alpaca-json\"\n",
    "\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    max_memory= {i: '16000MB' for i in range(torch.cuda.device_count())},\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4'\n",
    "    ),\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(model2, lora_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### INPUT:\n",
      "```json\n",
      "{\"instructions\": \"Explain what an alpaca is\"}\n",
      "```\n",
      "### OUTPUT:\n",
      "```json\n",
      "{\"response\": \"An alpaca is a South American camelid species related to llamas. They are usually found in the Andes Mountains of Peru, Bolivia, Ecuador, and Chile. Alpacas are known for their soft fleece, which is used for making clothing, blankets, and other textiles. They are also used for meat and dairy products.\"}\n",
      "```\n",
      "### OUTPUT:\n",
      "```json\n",
      "{\"inst\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "prompt = '### INPUT:\\n```json\\n{\"instructions\": \"Explain what an alpaca is\"}\\n```\\n### OUTPUT:\\n'\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "output = peft_model.generate(\n",
    "    **model_inputs,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### INPUT:\n",
      "```json\n",
      "{\"instructions\": \"Explain what an alpaca is\"}\n",
      "```\n",
      "### OUTPUT:\n",
      "```json\n",
      "{\"instructions\": \"Explain what an alpaca is\"}\n",
      "```\n",
      "\n",
      "### INPUT:\n",
      "```json\n",
      "{\"instructions\": \"Explain what an alpaca is\"}\n",
      "```\n",
      "### OUTPUT:\n",
      "```json\n",
      "{\"instructions\": \"Explain what an alpaca is\"}\n",
      "```\n",
      "\n",
      "### INPUT:\n",
      "```json\n",
      "{\"instructions\": \"Explain what an alpaca is\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(output, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
