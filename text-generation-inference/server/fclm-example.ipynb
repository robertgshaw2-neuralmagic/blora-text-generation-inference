{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/src/server-dev\n"
     ]
    }
   ],
   "source": [
    "%cd server-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make gen-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/src/server-dev/text_generation_server\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py   cache.py  interceptor.py  \u001b[0m\u001b[01;34mpb\u001b[0m/        tracing.py\n",
      "\u001b[01;34m__pycache__\u001b[0m/  cli.py    \u001b[01;34mmodels\u001b[0m/         server.py  \u001b[01;34mutils\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cd text_generation_server\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-17 15:57:28.180 | WARNING  | models:<module>:59 - Could not import Flash Attention enabled models: cannot import name 'FlashLlama' from partially initialized module 'text_generation_server.models.flash_llama' (most likely due to a circular import) (/usr/src/server-dev/text_generation_server/models/flash_llama.py)\n",
      "You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from text_generation_server.models.flash_llama import FlashLlama\n",
    "\n",
    "model = FlashLlama(model_id=\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slots = tensor([   0,    1,    2,  ..., 2701, 2702, 2703], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([2560, 4096])\n",
      "query.shape = torch.Size([2560, 32, 128])\n",
      "kv.shape = torch.Size([2560, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([170, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([170, 32, 128, 16])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from text_generation_server.models.flash_causal_lm import FlashCausalLMBatch\n",
    "from text_generation_server.pb import generate_pb2\n",
    "\n",
    "max_input_length = 256\n",
    "max_prefill_tokens = max_input_length*10\n",
    "\n",
    "warmup_requests = []\n",
    "n_tokens = 0\n",
    "while n_tokens < max_prefill_tokens:\n",
    "    warmup_requests.append(\n",
    "        generate_pb2.Request(\n",
    "            id=0,\n",
    "            inputs=\"_text\" * max_input_length,\n",
    "            truncate=min(max_input_length, max_prefill_tokens - n_tokens),\n",
    "            parameters=generate_pb2.NextTokenChooserParameters(\n",
    "                do_sample=False\n",
    "            ),\n",
    "            stopping_parameters=generate_pb2.StoppingCriteriaParameters(\n",
    "                max_new_tokens=2\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    n_tokens += max_input_length\n",
    "\n",
    "warmup_batch = generate_pb2.Batch(id=0, requests=warmup_requests, size=len(warmup_requests))\n",
    "\n",
    "fclm_warmup_batch = FlashCausalLMBatch.from_pb(\n",
    "    pb=warmup_batch,\n",
    "    tokenizer=model.tokenizer,\n",
    "    dtype=model.dtype,\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "max_supported_total_tokens = model.warmup(batch=fclm_warmup_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = generate_pb2.NextTokenChooserParameters(\n",
    "    watermark=False,\n",
    "    temperature=1.0,\n",
    "    repetition_penalty=1.0,\n",
    "    top_k=0,\n",
    "    top_p=1.0,\n",
    "    typical_p=1.0,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "stopping_parameters = generate_pb2.StoppingCriteriaParameters(\n",
    "    max_new_tokens=20,\n",
    "    ignore_eos_token=True\n",
    ")\n",
    "\n",
    "input_lst = [\n",
    "    \"The meaning of life is\",\n",
    "    \"Tell me a story about Barack Obama\",\n",
    "    \"Tell me a story about the Olympics\",\n",
    "    \"Tell me a story about Alexander the Great\"\n",
    "]\n",
    "\n",
    "requests = [\n",
    "    generate_pb2.Request(\n",
    "        id=idx,\n",
    "        inputs=inputs,\n",
    "        truncate=max_input_length,\n",
    "        parameters=parameters,    \n",
    "        stopping_parameters=stopping_parameters\n",
    "    )\n",
    "    for idx, inputs in enumerate(input_lst)\n",
    "]\n",
    "\n",
    "fclm_batch = FlashCausalLMBatch.from_pb(\n",
    "    pb=generate_pb2.Batch(id=0, requests=requests[:3]),\n",
    "    tokenizer=model.tokenizer,\n",
    "    dtype=model.dtype,\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "fclm_batch2 = FlashCausalLMBatch.from_pb(\n",
    "    pb=generate_pb2.Batch(id=1, requests=requests[-1:]),\n",
    "    tokenizer=model.tokenizer,\n",
    "    dtype=model.dtype,\n",
    "    device=model.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_slots: tensor([ 0, 25, 54])\n",
      "slot_indices: tensor([ 0,  1,  2,  3,  4,  5, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 54, 55,\n",
      "        56, 57, 58, 59, 60, 61], device='cuda:0')\n",
      "needed_blocks_slots: [(2, 25), (2, 29), (2, 27)]\n",
      "block_tables: None\n",
      "block_tables_tensor: None\n",
      "slots: None\n"
     ]
    }
   ],
   "source": [
    "relevant_keys = [\"start_slots\", \"start_inidices\", \"needed_blocks_slots\", \"block_tables\", \"block_tables_tensor\", \"slots\", \"slot_indices\"]\n",
    "\n",
    "def print_kv_cache_state(fclm_b):\n",
    "    for key in fclm_batch.__dict__.keys():\n",
    "        if key in relevant_keys:\n",
    "            print(f\"{key}: {getattr(fclm_batch, key)}\")\n",
    "\n",
    "print_kv_cache_state(fclm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slots = tensor([ 0,  1,  2,  3,  4,  5, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 64, 65,\n",
      "        66, 67, 68, 69, 70, 71], device='cuda:0', dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([24, 4096])\n",
      "query.shape = torch.Size([24, 32, 128])\n",
      "kv.shape = torch.Size([24, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([984, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([984, 32, 128, 16])\n",
      "\n",
      "\n",
      "slots = tensor([ 6, 42, 72], device='cuda:0', dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([3, 4096])\n",
      "query.shape = torch.Size([3, 32, 128])\n",
      "kv.shape = torch.Size([3, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([984, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([984, 32, 128, 16])\n",
      "\n",
      "\n",
      "slots = tensor([ 7, 43, 73], device='cuda:0', dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([3, 4096])\n",
      "query.shape = torch.Size([3, 32, 128])\n",
      "kv.shape = torch.Size([3, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([984, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([984, 32, 128, 16])\n",
      "\n",
      "\n",
      "slots = tensor([ 8, 44, 74], device='cuda:0', dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([3, 4096])\n",
      "query.shape = torch.Size([3, 32, 128])\n",
      "kv.shape = torch.Size([3, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([984, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([984, 32, 128, 16])\n",
      "\n",
      "\n",
      "slots = tensor([ 9, 45, 75], device='cuda:0', dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([3, 4096])\n",
      "query.shape = torch.Size([3, 32, 128])\n",
      "kv.shape = torch.Size([3, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([984, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([984, 32, 128, 16])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    generations, fclm_batch = model.generate_token(fclm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_slots: tensor([ 0, 25, 54])\n",
      "slot_indices: tensor([10, 39, 66], device='cuda:0')\n",
      "needed_blocks_slots: None\n",
      "block_tables: [[0, 1], [2, 3], [4, 5]]\n",
      "block_tables_tensor: tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]], device='cuda:0', dtype=torch.int32)\n",
      "slots: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,\n",
      "        43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n",
      "        64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90], device='cuda:0',\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print_kv_cache_state(fclm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_slots: tensor([ 0, 25])\n",
      "slot_indices: tensor([10, 37], device='cuda:0')\n",
      "needed_blocks_slots: None\n",
      "block_tables: [[0, 1], [4, 5]]\n",
      "block_tables_tensor: tensor([[0, 1],\n",
      "        [4, 5]], device='cuda:0', dtype=torch.int32)\n",
      "slots: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74,\n",
      "        75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "fclm_batch = fclm_batch.filter(request_ids=[0,2])\n",
    "print_kv_cache_state(fclm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slots = tensor([32, 33, 34, 35, 36, 37, 38, 39, 40], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([9, 4096])\n",
      "query.shape = torch.Size([9, 32, 128])\n",
      "kv.shape = torch.Size([9, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([984, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([984, 32, 128, 16])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generations, fclm_batch2 = model.generate_token(fclm_batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fclm_batch = FlashCausalLMBatch.concatenate(batches=[fclm_batch, fclm_batch2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_slots: tensor([ 0, 25, 52])\n",
      "slot_indices: tensor([10, 37, 61], device='cuda:0')\n",
      "needed_blocks_slots: None\n",
      "block_tables: [[0, 1], [4, 5], [2, 3]]\n",
      "block_tables_tensor: tensor([[0, 1],\n",
      "        [4, 5],\n",
      "        [2, 3]], device='cuda:0', dtype=torch.int32)\n",
      "slots: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74,\n",
      "        75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 32, 33,\n",
      "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
      "        52, 53, 54, 55, 56, 57, 58, 59], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print_kv_cache_state(fclm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slots = tensor([10, 76, 41], device='cuda:0', dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([3, 4096])\n",
      "query.shape = torch.Size([3, 32, 128])\n",
      "kv.shape = torch.Size([3, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([984, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([984, 32, 128, 16])\n",
      "\n",
      "\n",
      "slots = tensor([11, 77, 42], device='cuda:0', dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([3, 4096])\n",
      "query.shape = torch.Size([3, 32, 128])\n",
      "kv.shape = torch.Size([3, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([984, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([984, 32, 128, 16])\n",
      "\n",
      "\n",
      "slots = tensor([12, 78, 43], device='cuda:0', dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([3, 4096])\n",
      "query.shape = torch.Size([3, 32, 128])\n",
      "kv.shape = torch.Size([3, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([984, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([984, 32, 128, 16])\n",
      "\n",
      "\n",
      "slots = tensor([13, 79, 44], device='cuda:0', dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([3, 4096])\n",
      "query.shape = torch.Size([3, 32, 128])\n",
      "kv.shape = torch.Size([3, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([984, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([984, 32, 128, 16])\n",
      "\n",
      "\n",
      "slots = tensor([14, 80, 45], device='cuda:0', dtype=torch.int32)\n",
      "hidden_states.shape = torch.Size([3, 4096])\n",
      "query.shape = torch.Size([3, 32, 128])\n",
      "kv.shape = torch.Size([3, 2, 32, 128])\n",
      "kv_cache[0].shape = torch.Size([984, 32, 16, 16, 8])\n",
      "kv_cache[1].shape = torch.Size([984, 32, 128, 16])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    generations, fclm_batch = model.generate_token(fclm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_slots: tensor([ 0, 25, 52])\n",
      "slot_indices: tensor([15, 42, 66], device='cuda:0')\n",
      "needed_blocks_slots: None\n",
      "block_tables: [[0, 1], [4, 5], [2, 3]]\n",
      "block_tables_tensor: tensor([[0, 1],\n",
      "        [4, 5],\n",
      "        [2, 3]], device='cuda:0', dtype=torch.int32)\n",
      "slots: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74,\n",
      "        75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 32, 33,\n",
      "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
      "        52, 53, 54, 55, 56, 57, 58, 59], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print_kv_cache_state(fclm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
