{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Compile protos\n",
      "pip install grpcio-tools==1.51.1 mypy-protobuf==3.4.0 'types-protobuf>=3.20.4' --no-cache-dir\n",
      "Requirement already satisfied: grpcio-tools==1.51.1 in /opt/conda/lib/python3.9/site-packages (1.51.1)\n",
      "Requirement already satisfied: mypy-protobuf==3.4.0 in /opt/conda/lib/python3.9/site-packages (3.4.0)\n",
      "Requirement already satisfied: types-protobuf>=3.20.4 in /opt/conda/lib/python3.9/site-packages (4.24.0.1)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (4.24.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (68.1.2)\n",
      "Requirement already satisfied: grpcio>=1.51.1 in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (1.57.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mmkdir text_generation_server/pb || true\n",
      "mkdir: cannot create directory ‘text_generation_server/pb’: File exists\n",
      "python -m grpc_tools.protoc -I../proto --python_out=text_generation_server/pb \\\n",
      "\t--grpc_python_out=text_generation_server/pb --mypy_out=text_generation_server/pb ../proto/generate.proto\n",
      "/opt/conda/lib/python3.9/site-packages/grpc_tools/protoc.py:21: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "Writing mypy to generate_pb2.pyi\n",
      "find text_generation_server/pb/ -type f -name \"*.py\" -print0 -exec sed -i -e 's/^\\(import.*pb2\\)/from . \\1/g' {} \\;\n",
      "text_generation_server/pb/generate_pb2.py\u0000text_generation_server/pb/generate_pb2_grpc.py\u0000text_generation_server/pb/__init__.py\u0000touch text_generation_server/pb/__init__.py\n"
     ]
    }
   ],
   "source": [
    "!make gen-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/src/server-dev/text_generation_server\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py   cache.py  interceptor.py  \u001b[0m\u001b[01;34mpb\u001b[0m/        tracing.py\n",
      "\u001b[01;34m__pycache__\u001b[0m/  cli.py    \u001b[01;34mmodels\u001b[0m/         server.py  \u001b[01;34mutils\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cd text_generation_server\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from text_generation_server.models.flash_llama import FlashLlama\n",
    "\n",
    "model = FlashLlama(model_id=\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlashLlamaModel(\n",
       "  (embed_tokens): TensorParallelEmbedding()\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x FlashLlamaLayer(\n",
       "      (self_attn): FlashLlamaAttention(\n",
       "        (rotary_emb): PositionRotaryEmbedding()\n",
       "        (query_key_value): TensorParallelColumnLinear(\n",
       "          (linear): FastLinear()\n",
       "        )\n",
       "        (o_proj): TensorParallelRowLinear(\n",
       "          (linear): FastLinear()\n",
       "        )\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (act): SiLUActivation()\n",
       "        (gate_up_proj): TensorParallelColumnLinear(\n",
       "          (linear): FastLinear()\n",
       "        )\n",
       "        (down_proj): TensorParallelRowLinear(\n",
       "          (linear): FastLinear()\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.tuners.lora.LoraConfig"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft.mapping import (\n",
    "    MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n",
    "    PEFT_TYPE_TO_CONFIG_MAPPING,\n",
    ")\n",
    "\n",
    "PEFT_TYPE_TO_CONFIG_MAPPING[\"LORA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type='LORA', auto_mapping=None, base_model_name_or_path='/models/meta-llama--Llama-2-7b-hf', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=16, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft.peft_model import PeftModelForCausalLM\n",
    "from peft.tuners.lora import LoraConfig\n",
    "\n",
    "lora_id = \"nealchandra/llama-2-7b-hf-lora-alpaca-json\"\n",
    "\n",
    "lora_config = LoraConfig.from_pretrained(lora_id)\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation_server.utils.layers import (\n",
    "    TensorParallelColumnLinear,\n",
    "    TensorParallelRowLinear\n",
    ")\n",
    "\n",
    "FUSED_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    "\n",
    "def find_and_replace(model, lora_config):\n",
    "    assert lora_config.bias == \"none\"\n",
    "   \n",
    "    target_modules_non_fused = [\n",
    "        module for module in lora_config.target_modules\n",
    "        if module not in FUSED_MODULES\n",
    "    ]\n",
    "\n",
    "    target_modules_fused = [\n",
    "        module for module in lora_config.target_modules\n",
    "        if module in FUSED_MODULES\n",
    "    ]\n",
    "\n",
    "class BLoraTensorParallelColumnLinear(TensorParallelColumnLinear):\n",
    "\n",
    "class BLoraTensorParallelRowLinear:\n",
    "\n",
    "\n",
    "def create_new_module(model, lora_config, target):\n",
    "    bias = hasattr(target, \"bias\") and target.bias is not None\n",
    "    kwargs = {\n",
    "        \"r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"lora_dropout\": lora_config.lora_dropout,\n",
    "        \"fan_in_fan_out\": lora_config.fan_in_fan_out,\n",
    "        \"init_lora_weights\": lora_config.init_lora_weights,\n",
    "    }\n",
    "\n",
    "    if isinstance(target, TensorParallelColumnLinear):\n",
    "        pass\n",
    "    elif isinstance(target, TensorParallelRowLinear):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Target module {target} is not supported. \"\n",
    "            f\"Currently, only `TensorParallelColumnLinear` and `TensorParallelRowLinear` are supported.\"\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashLlamaAttention(\n",
      "  (rotary_emb): PositionRotaryEmbedding()\n",
      "  (query_key_value): TensorParallelColumnLinear(\n",
      "    (linear): FastLinear()\n",
      "  )\n",
      "  (o_proj): TensorParallelRowLinear(\n",
      "    (linear): FastLinear()\n",
      "  )\n",
      ")\n",
      "TensorParallelRowLinear(\n",
      "  (linear): FastLinear()\n",
      ")\n",
      "o_proj\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/blora-text-generation-inference/text-generation-inference/server/text_generation_server/models/blora-dev.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B18.215.173.205/home/ubuntu/blora-text-generation-inference/text-generation-inference/server/text_generation_server/models/blora-dev.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(target)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B18.215.173.205/home/ubuntu/blora-text-generation-inference/text-generation-inference/server/text_generation_server/models/blora-dev.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(target_name)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B18.215.173.205/home/ubuntu/blora-text-generation-inference/text-generation-inference/server/text_generation_server/models/blora-dev.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from peft.tuners.lora import LoraModel\n",
    "from peft.utils.other import _get_submodules\n",
    "\n",
    "key_list = [key for key, _ in model.model.named_modules()]\n",
    "\n",
    "for key in key_list:\n",
    "    if not LoraModel._check_target_module_exists(None, lora_config, key):\n",
    "        pass\n",
    "    else:\n",
    "        parent, target, target_name = _get_submodules(model.model, key)\n",
    "        print(parent)\n",
    "        print(target)\n",
    "        print(target_name)\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_proj', 'k_proj', 'v_proj', 'o_proj']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config.target_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation_server.models import FlashLlama\n",
    "from typing import Dict\n",
    "\n",
    "class BLoraFlashLlama(FlashLlama):\n",
    "    def __init__(\n",
    "        model,\n",
    "        config\n",
    "    ):\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
