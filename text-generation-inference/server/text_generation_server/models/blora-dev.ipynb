{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/src/server-dev\n"
     ]
    }
   ],
   "source": [
    "%cd server-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Compile protos\n",
      "pip install grpcio-tools==1.51.1 mypy-protobuf==3.4.0 'types-protobuf>=3.20.4' --no-cache-dir\n",
      "Requirement already satisfied: grpcio-tools==1.51.1 in /opt/conda/lib/python3.9/site-packages (1.51.1)\n",
      "Requirement already satisfied: mypy-protobuf==3.4.0 in /opt/conda/lib/python3.9/site-packages (3.4.0)\n",
      "Requirement already satisfied: types-protobuf>=3.20.4 in /opt/conda/lib/python3.9/site-packages (4.23.0.2)\n",
      "Requirement already satisfied: grpcio>=1.51.1 in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (1.56.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (68.0.0)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (4.23.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mmkdir text_generation_server/pb || true\n",
      "mkdir: cannot create directory ‘text_generation_server/pb’: File exists\n",
      "python -m grpc_tools.protoc -I../proto --python_out=text_generation_server/pb \\\n",
      "\t--grpc_python_out=text_generation_server/pb --mypy_out=text_generation_server/pb ../proto/generate.proto\n",
      "/opt/conda/lib/python3.9/site-packages/grpc_tools/protoc.py:21: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "Writing mypy to generate_pb2.pyi\n",
      "find text_generation_server/pb/ -type f -name \"*.py\" -print0 -exec sed -i -e 's/^\\(import.*pb2\\)/from . \\1/g' {} \\;\n",
      "text_generation_server/pb/generate_pb2.py\u0000text_generation_server/pb/generate_pb2_grpc.py\u0000text_generation_server/pb/__init__.py\u0000touch text_generation_server/pb/__init__.py\n"
     ]
    }
   ],
   "source": [
    "!make gen-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from text_generation_server.models.flash_llama import FlashLlama\n",
    "\n",
    "model = FlashLlama(model_id=\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft.tuners.lora import LoraConfig\n",
    "from text_generation_server.utils.layers import SuperLayer, TensorParallelColumnLinear, TensorParallelRowLinear\n",
    "from text_generation_server.utils import (\n",
    "    weight_files,\n",
    "    Weights,\n",
    ")\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class BLoraConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lora_id: str,\n",
    "        lora_r: int,\n",
    "        lora_alpha: int,\n",
    "        weights: Weights,\n",
    "    ):\n",
    "        self.lora_id = lora_id\n",
    "        self.lora_r = lora_r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.weights = weights\n",
    "\n",
    "class BLoraLinear(torch.nn.Module):\n",
    "    def __init__(self, linear, r, target_modules) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.r = r\n",
    "        self.target_modules = target_modules\n",
    "\n",
    "        # adapter weights\n",
    "        self.lora_ids = {target_module: set() for target_module in self.target_modules}\n",
    "        self.scales = {target_module: {} for target_module in self.target_modules}\n",
    "        self.lora_A = {target_module: {} for target_module in self.target_modules}\n",
    "        self.lora_B = {target_module: {} for target_module in self.target_modules}\n",
    "\n",
    "        # adapter weights in batch format\n",
    "        self.batch_lora_ids = {target_module: [] for target_module in self.target_modules}\n",
    "        self.scales_batch = {target_module: None for target_module in self.target_modules}\n",
    "        self.lora_A_batch = {target_module: None for target_module in self.target_modules}\n",
    "        self.lora_B_batch = {target_module: None for target_module in self.target_modules}\n",
    "\n",
    "    def load_adapter(\n",
    "        self, \n",
    "        lora_id: str, \n",
    "        lora_alpha: int, \n",
    "        weights: Dict[str, Tuple[torch.Tensor, torch.Tensor]],\n",
    "    ):\n",
    "        # confirm adapters passed are all Wq,Wk,Wv\n",
    "        if len(weights) != len(self.target_modules):\n",
    "            raise NotImplementedError(\"Currently require adapter for all of sub-matrices\")\n",
    "        \n",
    "        # actually load the data\n",
    "        for target_module in weights:\n",
    "            if target_module not in self.target_modules:\n",
    "                raise ValueError(f\"Module passed to load_adapter must be in {self.target_modules}\")\n",
    "            if lora_id in self.lora_ids[target_module]:\n",
    "                raise ValueError(f\"{lora_id} already loaded into this module\")\n",
    "            \n",
    "            self.lora_ids[target_module].add(lora_id)\n",
    "            self.scales[target_module][lora_id] = lora_alpha / self.r\n",
    "            self.lora_A[target_module][lora_id] = weights[target_module][0].T\n",
    "            self.lora_B[target_module][lora_id] = weights[target_module][1].T\n",
    "    \n",
    "    def set_batch_lora_ids(self, lora_ids: List[str]):\n",
    "        for target_module in self.target_modules:\n",
    "            for lora_id in lora_ids:\n",
    "                if lora_id not in self.lora_ids[target_module]:\n",
    "                    raise NotImplementedError(\"Not yet handling some items in batch not having an adapter\")\n",
    "                \n",
    "            self.batch_lora_ids[target_module] = lora_ids\n",
    "\n",
    "        # create the tensors [lora_b, W]\n",
    "        # TODO: figure out how to get this on the right device in sharded mode\n",
    "        for target_module in self.target_modules:\n",
    "            self.lora_A_batch[target_module] = torch.stack([self.lora_A[target_module][lora_id] for lora_id in self.batch_lora_ids])\n",
    "            self.lora_B_batch[target_module] = torch.stack([self.lora_B[target_module][lora_id] for lora_id in self.batch_lora_ids])\n",
    "            self.scales_batch[target_module] = torch.tensor([self.scales[target_module][lora_id] for lora_id in self.batch_lora_ids]).reshape(-1,1,1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        previous_dtype = x.dtype\n",
    "        \n",
    "        # xW\n",
    "        out = self.linear(x)\n",
    "\n",
    "        # xAB\n",
    "        for target_module in enumerate(self.target_modules):\n",
    "            if x.shape[0] != len(self.batch_lora_ids[target_module]):\n",
    "                raise NotImplementedError(\"Not yet handling some items in batch not having an adapter\")\n",
    "            self.lora_forward(out, x)\n",
    "        \n",
    "        return out.to(previous_dtype)\n",
    "    \n",
    "    def lora_forward(self, out: torch.Tensor, x: torch.Tensor, target_module: str):\n",
    "        out += torch.bmm(torch.bmm(x, self.lora_A_batch[target_module]), self.lora_A_batch[target_module])\n",
    "\n",
    "class BLoraLinearQKV(BLoraLinear):\n",
    "    def __init__(self, linear, r, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]) -> None:\n",
    "        super().__init__(linear, r, target_modules)\n",
    "\n",
    "        # get endpoints of Wq, Wk, Wv\n",
    "        combined_width = self.linear.weight.shape[0]\n",
    "        if combined_width != 4096 * 3:\n",
    "            raise NotImplementedError(\"Currently requires all Wq, Wk, Wk to be the same size\")\n",
    "        width = combined_width // 3\n",
    "        self.start_out_indexes = {target_module: idx * width for idx, target_module in enumerate(self.target_modules)}\n",
    "        self.end_out_indexes = {target_module: (idx + 1) * width for idx, target_module in enumerate(self.target_modules)}\n",
    "\n",
    "    def lora_forward(self, out: torch.Tensor, x: torch.Tensor, target_module: str):\n",
    "        start = self.start_out_indexes[target_module]\n",
    "        end = self.end_out_indexes[target_module]\n",
    "        \n",
    "        out[:, start: end] += torch.bmm(torch.bmm(x, self.lora_A_batch[target_module]), self.lora_A_batch[target_module])\n",
    "\n",
    "class BLoraTensorParallelColumnLinear(SuperLayer):\n",
    "    def __init__(self, linear):\n",
    "        super().__init__(linear)\n",
    "\n",
    "    @classmethod\n",
    "    def from_linear(\n",
    "        cls, \n",
    "        linear: TensorParallelColumnLinear,\n",
    "        prefix: str,\n",
    "        lora_r: int,\n",
    "        lora_configs: List[BLoraConfig],\n",
    "        target_modules: List[str],\n",
    "    ):\n",
    "        # SETUP WRAPPER\n",
    "        blora_linear = BLoraLinearQKV(\n",
    "            linear=linear.linear,\n",
    "            r=lora_r,\n",
    "            target_modules=target_modules\n",
    "        )\n",
    "\n",
    "        # LOAD WEIGHTS INTO MEMORY\n",
    "        for lora_config in lora_configs:\n",
    "            adapter_weights = {}\n",
    "\n",
    "            for target_module in target_modules:\n",
    "                weight_A = lora_config.weights.get_multi_weights_col(\n",
    "                    prefixes=[f\"base_model.model.{prefix}.{target_module}.lora_A\"], \n",
    "                    quantize=None,\n",
    "                    dim=0\n",
    "                )\n",
    "                weight_B = lora_config.weights.get_multi_weights_col(\n",
    "                    prefixes=[f\"base_model.model.{prefix}.{target_module}.lora_B\"],\n",
    "                    quantize=None,\n",
    "                    dim=0\n",
    "                )\n",
    "\n",
    "                adapter_weights[target_module] = (weight_A, weight_B)\n",
    "            \n",
    "            if lora_r != lora_config.lora_r:\n",
    "                raise ValueError(\"All LORA adapters must have the same rank\")\n",
    "            \n",
    "            # SETUP ADAPTER\n",
    "            blora_linear.load_adapter(\n",
    "                lora_id=lora_config.lora_id,\n",
    "                lora_alpha=lora_config.lora_alpha,\n",
    "                weights=adapter_weights,\n",
    "            )\n",
    "        \n",
    "        return cls(blora_linear)\n",
    "    \n",
    "class BLoraTensorParallelRowLinear(SuperLayer):\n",
    "    def __init__(self, linear, process_group):\n",
    "        if process_group.size() > 1:\n",
    "            raise NotImplementedError(\"Currently not supporting sharded\")\n",
    "        \n",
    "        super().__init__(linear)\n",
    "        self.process_group = process_group\n",
    "\n",
    "    @classmethod\n",
    "    def from_linear(\n",
    "        cls, \n",
    "        linear: TensorParallelRowLinear,\n",
    "        prefix: str,\n",
    "        lora_r: int,\n",
    "        lora_configs: List[BLoraConfig],\n",
    "        target_modules: List[str],\n",
    "    ):  \n",
    "        # SETUP WRAPPER\n",
    "        blora_linear = BLoraLinear(\n",
    "            linear=linear.linear,\n",
    "            r=lora_r,\n",
    "            target_modules=target_modules\n",
    "        )\n",
    "\n",
    "        # LOAD WEIGHTS INTO MEMORY\n",
    "        for lora_config in lora_configs:\n",
    "            adapter_weights = {}\n",
    "\n",
    "            for target_module in target_modules:\n",
    "                weight_A = lora_config.weights.get_multi_weights_row(\n",
    "                    prefix=f\"base_model.model.{prefix}.{target_module}.lora_A\", \n",
    "                    quantize=None\n",
    "                )\n",
    "                weight_B = lora_config.weights.get_multi_weights_row(\n",
    "                    prefix=f\"base_model.model.{prefix}.{target_module}.lora_B\", \n",
    "                    quantize=None\n",
    "                )\n",
    "\n",
    "                adapter_weights[target_module] = (weight_A, weight_B)\n",
    "\n",
    "            if lora_r != lora_config.lora_r:\n",
    "                raise ValueError(\"All LORA adapters must have the same rank\")\n",
    "            \n",
    "            # SETUP ADAPTER\n",
    "            blora_linear.load_adapter(\n",
    "                lora_id=lora_config.lora_id,\n",
    "                lora_alpha=lora_config.lora_alpha,\n",
    "                weights=adapter_weights,\n",
    "            )\n",
    "        \n",
    "        return cls(blora_linear, process_group=linear.process_group)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        out = super().forward(input)\n",
    "        if self.process_group.size() > 1:\n",
    "            torch.distributed.all_reduce(out, group=self.process_group)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLoraFlashLlama:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        lora_configs: Dict[str, LoraConfig],\n",
    "        lora_r=16,\n",
    "    ):\n",
    "        self.model = model\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "        # format blora configs\n",
    "        blora_configs = []\n",
    "        for lora_id, lora_config in lora_configs.items():    \n",
    "            # error checking\n",
    "            if set(lora_config.target_modules) != set(target_modules):\n",
    "                raise NotImplementedError(\n",
    "                    \"\"\"\n",
    "                    Currently require lora adapters on exactly {self.target_modules}\n",
    "                    \"\"\"\n",
    "                )\n",
    "            \n",
    "            if lora_config.r != lora_r:\n",
    "                raise ValueError(\n",
    "                    \"\"\"\n",
    "                    Currently require all lora adapters to have the same r. lora_config.r={lora_config.r} / lora_r ={lora_r}\n",
    "                    \"\"\"\n",
    "                )\n",
    "\n",
    "            filenames = weight_files(lora_id, extension=\".safetensors\")\n",
    "            if len(filenames) < 1:\n",
    "                raise ValueError(\n",
    "                    \"\"\"\n",
    "                    Weight files not found for LORA adapter. Make sure you download with \n",
    "                    text-generation-server download-weights {lora_id}\n",
    "                    \"\"\"\n",
    "                )\n",
    "            \n",
    "            # unpack configurations \n",
    "            blora_configs.append(BLoraConfig(\n",
    "                lora_id=lora_id,\n",
    "                lora_r=lora_config.r,\n",
    "                lora_alpha=lora_config.lora_alpha,\n",
    "                weights=Weights(\n",
    "                    filenames, \n",
    "                    self.model.device, \n",
    "                    dtype=self.model.dtype, \n",
    "                    process_group=self.model.process_group\n",
    "                ),\n",
    "            ))\n",
    "        \n",
    "        # update layers\n",
    "        for layer_id, layer in enumerate(self.model.model.model.layers):\n",
    "            prefix = f\"model.layers.{layer_id}.self_attn\"\n",
    "\n",
    "            # update q_proj, k_proj, v_proj\n",
    "            if not isinstance(layer.self_attn.query_key_value, TensorParallelColumnLinear):\n",
    "                print(layer.self_attn.query_key_value)\n",
    "                raise ValueError(\"Expected query_key_value to be TensorParallelColumnLinear\")\n",
    "\n",
    "            layer.self_attn.query_key_value = BLoraTensorParallelColumnLinear.from_linear(\n",
    "                linear=layer.self_attn.query_key_value,\n",
    "                prefix=prefix,\n",
    "                lora_r=lora_r,\n",
    "                lora_configs=blora_configs,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    "            )\n",
    "\n",
    "            # update o_proj\n",
    "            if not isinstance(layer.self_attn.o_proj, TensorParallelRowLinear):\n",
    "                print(layer)\n",
    "                raise ValueError(\"Expected o_proj to be TensorParallelRowLinear\")\n",
    "            \n",
    "            layer.self_attn.o_proj = BLoraTensorParallelRowLinear.from_linear(\n",
    "                linear=layer.self_attn.o_proj,\n",
    "                prefix=prefix,\n",
    "                lora_r=lora_r,\n",
    "                lora_configs=blora_configs,\n",
    "                target_modules=[\"o_proj\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_id = \"nealchandra/llama-2-7b-hf-lora-alpaca-json\"\n",
    "lora_config = LoraConfig.from_pretrained(lora_id)\n",
    "\n",
    "blora_llama = BLoraFlashLlama(model, {lora_id:lora_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(blora_llama.model.model.model.layers[0].self_attn.o_proj.linear.lora_B[\"o_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 16])\n",
      "torch.Size([16, 4096])\n",
      "torch.Size([4096, 16])\n",
      "torch.Size([16, 4096])\n",
      "torch.Size([4096, 16])\n",
      "torch.Size([16, 4096])\n",
      "torch.Size([4096, 16])\n",
      "torch.Size([16, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(blora_llama.model.model.model.layers[0].self_attn.o_proj.linear.lora_A[\"o_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.o_proj.linear.lora_B[\"o_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.lora_A[\"q_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.lora_B[\"q_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.lora_A[\"k_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.lora_B[\"k_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.lora_A[\"v_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.lora_B[\"v_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
