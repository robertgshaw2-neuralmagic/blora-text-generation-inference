{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_MEMORY_FRACTION\"] = \"0.97\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/src/server-dev\n",
      "# Compile protos\n",
      "pip install grpcio-tools==1.51.1 mypy-protobuf==3.4.0 'types-protobuf>=3.20.4' --no-cache-dir\n",
      "Requirement already satisfied: grpcio-tools==1.51.1 in /opt/conda/lib/python3.9/site-packages (1.51.1)\n",
      "Requirement already satisfied: mypy-protobuf==3.4.0 in /opt/conda/lib/python3.9/site-packages (3.4.0)\n",
      "Requirement already satisfied: types-protobuf>=3.20.4 in /opt/conda/lib/python3.9/site-packages (4.23.0.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (68.0.0)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (4.23.3)\n",
      "Requirement already satisfied: grpcio>=1.51.1 in /opt/conda/lib/python3.9/site-packages (from grpcio-tools==1.51.1) (1.56.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mmkdir text_generation_server/pb || true\n",
      "mkdir: cannot create directory ‘text_generation_server/pb’: File exists\n",
      "python -m grpc_tools.protoc -I../proto --python_out=text_generation_server/pb \\\n",
      "\t--grpc_python_out=text_generation_server/pb --mypy_out=text_generation_server/pb ../proto/generate.proto\n",
      "/opt/conda/lib/python3.9/site-packages/grpc_tools/protoc.py:21: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "Writing mypy to generate_pb2.pyi\n",
      "find text_generation_server/pb/ -type f -name \"*.py\" -print0 -exec sed -i -e 's/^\\(import.*pb2\\)/from . \\1/g' {} \\;\n",
      "text_generation_server/pb/generate_pb2.py\u0000text_generation_server/pb/generate_pb2_grpc.py\u0000text_generation_server/pb/__init__.py\u0000touch text_generation_server/pb/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%cd server-dev\n",
    "!make gen-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from text_generation_server.utils.blora import BLoraConfig, BLoraTensorParallelColumnLinear, BLoraTensorParallelRowLinear\n",
    "from text_generation_server.utils import weight_files, Weights\n",
    "from text_generation_server.utils.layers import TensorParallelColumnLinear, TensorParallelRowLinear\n",
    "from typing import Dict\n",
    "\n",
    "class BLoraFlashLlama:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        lora_configs: Dict[str, LoraConfig],\n",
    "        lora_r=16,\n",
    "    ):\n",
    "        self.model = model\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "        # format blora configs\n",
    "        blora_configs = []\n",
    "        for lora_id, lora_config in lora_configs.items():    \n",
    "            # error checking\n",
    "            if set(lora_config.target_modules) != set(target_modules):\n",
    "                raise NotImplementedError(\n",
    "                    \"\"\"\n",
    "                    Currently require lora adapters on exactly {self.target_modules}\n",
    "                    \"\"\"\n",
    "                )\n",
    "            \n",
    "            if lora_config.r != lora_r:\n",
    "                raise ValueError(\n",
    "                    \"\"\"\n",
    "                    Currently require all lora adapters to have the same r. lora_config.r={lora_config.r} / lora_r ={lora_r}\n",
    "                    \"\"\"\n",
    "                )\n",
    "\n",
    "            filenames = weight_files(lora_id, extension=\".safetensors\")\n",
    "            if len(filenames) < 1:\n",
    "                raise ValueError(\n",
    "                    \"\"\"\n",
    "                    Weight files not found for LORA adapter. Make sure you download with \n",
    "                    text-generation-server download-weights {lora_id}\n",
    "                    \"\"\"\n",
    "                )\n",
    "            \n",
    "            # unpack configurations \n",
    "            blora_configs.append(BLoraConfig(\n",
    "                lora_id=lora_id,\n",
    "                lora_r=lora_config.r,\n",
    "                lora_alpha=lora_config.lora_alpha,\n",
    "                weights=Weights(\n",
    "                    filenames, \n",
    "                    self.model.device, \n",
    "                    dtype=self.model.dtype, \n",
    "                    process_group=self.model.process_group\n",
    "                ),\n",
    "            ))\n",
    "        \n",
    "        # update layers\n",
    "        for layer_id, layer in enumerate(self.model.model.model.layers):\n",
    "            prefix = f\"model.layers.{layer_id}.self_attn\"\n",
    "\n",
    "            # update q_proj, k_proj, v_proj\n",
    "            if not isinstance(layer.self_attn.query_key_value, TensorParallelColumnLinear):\n",
    "                print(layer.self_attn.query_key_value)\n",
    "                raise ValueError(\"Expected query_key_value to be TensorParallelColumnLinear\")\n",
    "\n",
    "            layer.self_attn.query_key_value = BLoraTensorParallelColumnLinear.from_linear(\n",
    "                linear=layer.self_attn.query_key_value,\n",
    "                prefix=prefix,\n",
    "                lora_r=lora_r,\n",
    "                lora_configs=blora_configs,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    "            )\n",
    "\n",
    "            # update o_proj\n",
    "            if not isinstance(layer.self_attn.o_proj, TensorParallelRowLinear):\n",
    "                print(layer)\n",
    "                raise ValueError(\"Expected o_proj to be TensorParallelRowLinear\")\n",
    "            \n",
    "            layer.self_attn.o_proj = BLoraTensorParallelRowLinear.from_linear(\n",
    "                linear=layer.self_attn.o_proj,\n",
    "                prefix=prefix,\n",
    "                lora_r=lora_r,\n",
    "                lora_configs=blora_configs,\n",
    "                target_modules=[\"o_proj\"],\n",
    "            )\n",
    "    \n",
    "    def set_batch_ids(self, lora_ids, cu_seqlen_prefill):\n",
    "        for layer in self.model.model.model.layers:\n",
    "            layer.self_attn.query_key_value.linear.set_batch_lora_ids(lora_ids, cu_seqlen_prefill)\n",
    "            layer.self_attn.o_proj.linear.set_batch_lora_ids(lora_ids, cu_seqlen_prefill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from text_generation_server.models.flash_llama import FlashLlama\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "lora_id = \"nealchandra/llama-2-7b-hf-lora-alpaca-json\"\n",
    "\n",
    "model = FlashLlama(model_id=model_id, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 16])\n",
      "torch.Size([16, 4096])\n",
      "torch.Size([4096, 16])\n",
      "torch.Size([16, 4096])\n"
     ]
    }
   ],
   "source": [
    "blora_llama = BLoraFlashLlama(model, {lora_id: LoraConfig.from_pretrained(lora_id)})\n",
    "\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.o_proj.linear.lora_A[\"o_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.o_proj.linear.lora_B[\"o_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.lora_A[\"q_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)\n",
    "print(blora_llama.model.model.model.layers[0].self_attn.query_key_value.linear.lora_B[\"q_proj\"][\"nealchandra/llama-2-7b-hf-lora-alpaca-json\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation_server.models.flash_causal_lm import FlashCausalLMBatch\n",
    "from text_generation_server.pb import generate_pb2\n",
    "\n",
    "max_input_length = 128\n",
    "max_batch_size = 2\n",
    "max_prefill_tokens = max_input_length * max_batch_size - 32\n",
    "\n",
    "warmup_requests = []\n",
    "n_tokens = 0\n",
    "while n_tokens < max_prefill_tokens:\n",
    "    warmup_requests.append(\n",
    "        generate_pb2.Request(\n",
    "            id=0,\n",
    "            inputs=\"_text\" * max_input_length,\n",
    "            truncate=min(max_input_length, max_prefill_tokens - n_tokens),\n",
    "            parameters=generate_pb2.NextTokenChooserParameters(\n",
    "                do_sample=False\n",
    "            ),\n",
    "            stopping_parameters=generate_pb2.StoppingCriteriaParameters(\n",
    "                max_new_tokens=2\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    n_tokens += max_input_length\n",
    "\n",
    "warmup_batch = generate_pb2.Batch(id=0, requests=warmup_requests, size=len(warmup_requests))\n",
    "\n",
    "fclm_warmup_batch = FlashCausalLMBatch.from_pb(\n",
    "    pb=warmup_batch,\n",
    "    tokenizer=model.tokenizer,\n",
    "    dtype=model.dtype,\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "blora_llama.set_batch_ids([lora_id] * max_batch_size, cu_seqlen_prefill=fclm_warmup_batch.cu_seqlen_prefill)\n",
    "max_supported_total_tokens = blora_llama.model.warmup(batch=fclm_warmup_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = generate_pb2.NextTokenChooserParameters(\n",
    "    watermark=False,\n",
    "    temperature=1.0,\n",
    "    repetition_penalty=1.0,\n",
    "    top_k=0,\n",
    "    top_p=1.0,\n",
    "    typical_p=1.0,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "stopping_parameters = generate_pb2.StoppingCriteriaParameters(\n",
    "    max_new_tokens=100,\n",
    "    ignore_eos_token=True\n",
    ")\n",
    "\n",
    "input_lst = [\n",
    "    '### INPUT:\\n```json\\n{\"instructions\": \"Explain what an alpaca is\"}\\n```\\n### OUTPUT:\\n',\n",
    "    '### INPUT:\\n```json\\n{\"instructions\": \"Describe what deep learning is\"}\\n```\\n### OUTPUT:\\n'\n",
    "]\n",
    "\n",
    "requests = [\n",
    "    generate_pb2.Request(\n",
    "        id=idx,\n",
    "        inputs=inputs,\n",
    "        truncate=max_input_length,\n",
    "        parameters=parameters,    \n",
    "        stopping_parameters=stopping_parameters\n",
    "    )\n",
    "    for idx, inputs in enumerate(input_lst)\n",
    "]\n",
    "\n",
    "fclm_batch = FlashCausalLMBatch.from_pb(\n",
    "    pb=generate_pb2.Batch(id=0, requests=requests),\n",
    "    tokenizer=model.tokenizer,\n",
    "    dtype=model.dtype,\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "blora_llama.set_batch_ids([lora_id] * len(requests), cu_seqlen_prefill=fclm_batch.cu_seqlen_prefill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {\n",
    "    idx: request.inputs\n",
    "    for idx, request in enumerate(fclm_batch.requests)\n",
    "}\n",
    "    \n",
    "for _ in range(99):\n",
    "    generations, fclm_batch = model.generate_token(fclm_batch)\n",
    "    for idx, gen in enumerate(generations):\n",
    "        texts[idx] += gen.token_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### INPUT:\n",
      "```json\n",
      "{\"instructions\": \"Explain what an alpaca is\"}\n",
      "```\n",
      "### OUTPUT:\n",
      "```json\n",
      "{\"response\": \"An alpaca is a type of South American camelid that is related to llamas. Alpacas are known for their soft fleece, which is used to make clothing, blankets, and other textiles. They are also used for their meat and for their wool, which is used to make yarn and other textiles.\"}\n",
      "```\n",
      "### INPUT:\n",
      "```json\n",
      "{\"instructions\":\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### INPUT:\n",
      "```json\n",
      "{\"instructions\": \"Describe what deep learning is\"}\n",
      "```\n",
      "### OUTPUT:\n",
      "```json\n",
      "{\"response\": \"Deep learning is a subset of machine learning that uses artificial neural networks to learn from large amounts of data. It is a type of machine learning that uses multiple layers of artificial neurons to process data and make decisions. Deep learning algorithms are able to learn complex patterns in data and make predictions with high accuracy.\"}\n",
      "```\n",
      "### OUTPUT:\n",
      "```json\n",
      "{\"instructions\": \"Describe the benefits of deep learning.\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
